---
title: "Comparison of IIT only with IIT with Mn & C"
author: "RSI PIpeline Solutions"
date: "`r format(Sys.time(), '%d %B, %Y')`" 

output: bookdown::word_document2

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = 'center' )
```

## Executive Summary {-}
The goal of this to study is to compare the yield strength (YS) predictions using Instrumented Indentation Testing (IIT) by itself and IIT combined with composition elements as additional predictors. IIT follows a known stress-strain relationship but like any physical measurement there are uncertainties involved that lead to a variation in predictions vs. the observed destructive YS. Since these predictions are being used to make judgement about the suitability of MAOP for unknown materials, it is desirable to reduce this model variance to a minimum. As part of this model improvement process it was hypothesized that the inclusion of certain elements that are known to have influence on yield strength as additional predictors could be beneficial.

After exploring several combinations of elements it was decided to include Mn and C as additional predictors. These additional factors increased the $R^2$ from a 0.67 to 0.81 and more importantly reduced the standard deviation of the residuals by 25%.

This report will explore the YS predictions from IIT testing used by itself and IIT combined with different composition elements, specifically Mn (Mn) and Carbon (C).  The two models will be referred to as "IIT Only" and the "IIT + Composition".

## Regression Analysis  {-}
For this analysis, it was chosen to use least squares regression. Regression analysis was chosen because it is a robust technique and is the most interpretable of all machine learning techniques. Once the regression analysis is complete, the coefficients can be applied to new data without having to apply a machine learning model.

Regression analysis makes certain assumptions about the data, specifically that all the residuals are independent of each other, the errors are normally distributed with a mean of zero and that effects are additive for each predictor.

## IIT Only Model {-}
The initial step this process was to review the IIT measurements relative to the destructive yield strength (YS). Although both the IIT and destructive yield strength have uncertainties, the destructive yield strength is considered the "truth" for all the analysis since the uncertainty is much less than the IIT. The plot of destructive YS vs. IIT is shown in Figure \@ref(fig:iit-only). The best fit line of the observations appear to have a slight bias to under-predicting as witnessed by the parallel offset to the 1:1 line.  The amount of under-prediction is more prominent in the higher YS values. 

```{r data_load, message = FALSE}
# compostion data ---------------------------------------------------------
library(readxl)
library(tidyverse)
library(tidymodels)
library(performance)
library(gtsummary)
library(patchwork)

theme_set(theme_bw(14, "serif"))

lab <-
  c("Anamet",
    "Bryan Labs",
    "K Prime",
    "LTI",
    "MTR")


comp <-
  read_excel("~/RSI/ChemistryGrade/MasterDB-SQL-2020-09-24.xlsx",
             sheet = "Composition") %>%
  janitor::clean_names() %>%
  filter(skip == FALSE) %>%
  select(group, feature,  mn, c, si) %>%
  group_by(group, feature) %>%
  mutate(across(.cols = mn:si, as.numeric)) %>%
  summarise(across(.cols = mn:si, ~ mean(.x, na.rm = T))) %>%
  ungroup()

comp2 <- read_excel("~/RSI/ChemistryGrade/MasterDB-SQL-2020-09-24.xlsx",
             sheet = "Composition") %>%
  janitor::clean_names() %>%
  filter(skip == FALSE) %>%
  mutate(lab_nde = ifelse(vendor %in% lab,"lab","nde")) %>% 
  select(group, feature,  mn, c, lab_nde) %>%
  group_by(group, feature,lab_nde) %>%
  mutate(across(.cols = mn:c, as.numeric)) %>%
  summarise(across(.cols = mn:c, ~ mean(.x, na.rm = T))) %>%
  ungroup()

# IIT data ----------------------------------------------------------------

ndt <-
  read_excel("~/RSI/ChemistryGrade/MasterDB-SQL-2020-09-24.xlsx",
             sheet = "NDT") %>%
  janitor::clean_names() %>%
  rename(group = group_name) %>%
  filter(
    skip == FALSE,
    group != "Calibration Blocks",
    str_detect("Old",
               negate = T,
               vendor_test_area_label),
    reader_method == "DPT",
    vendor %in% c('ATS', 'TDW')
  ) %>%
  mutate(lab_nde = ifelse(vendor %in% lab, "lab", "nde")) %>%
  group_by(group, feature) %>%
  summarise(
    iitys_5 = mean(x0_5_percent_eul_ys_ksi, na.rm = T),
    iituts = mean(uts_ksi, na.rm = T),
    nsamples = n()
  ) %>%
  filter(nsamples >= 10) %>%
  ungroup()


# Tensile data ------------------------------------------------------------


tensile <-
  read_excel("~/RSI/ChemistryGrade/MasterDB-SQL-2020-09-24.xlsx",
             sheet = "Tensile") %>%
  janitor::clean_names() %>%
  filter(skip == FALSE ,
         type != "Weld" ,
         type != "GirthWeldStrip" ,
         group != "Calibration Blocks") %>%
  rename(ys5 = x0_5_percent_eul_ys_ksi,
         uts = uts_ksi) %>%
  group_by(group, feature) %>%
  summarise(ten_ys = mean(ys5, na.rm = T),
            ten_uts = mean(uts, na.rm = T)) %>%
  ungroup()


## Combine the data -----------------------------------------------------
ndt_ten_comp <- ndt %>%
  full_join(comp,
            by = c("group", "feature")) %>%
  full_join(tensile,
            by = c("group", "feature")) %>%
  drop_na()


```

```{r iit-only, fig.cap="IIT only Model"}

ndt_ten_comp %>%
  # pivot_longer(cols = c(.fitted_iit, .fitted_comp)) %>%
  ggplot(aes(y = ten_ys, x = iitys_5)) +
  geom_jitter(
    width = 0.1,
    height = 0.1,
    alpha = 0.65,
    col = 'midnightblue'
  ) +
  coord_obs_pred() +
  geom_abline(lty = 2, col = 'grey50', lwd = 0.7) +
  geom_smooth(method = "lm", se = F) +
  scale_color_brewer(type = "div", palette = "Set1") +
  labs(title = "Observed vs. IIT Measurement",
       x = "IIT Measured YS (ksi)",
       y = "Destructive YS (ksi)")

```
  
After the initial exploration of the IIT values vs YS, a regression model was fit to the data and the results plotted in Figure \@ref(fig:iit-regress). The regression model and plot of results show significant scatter to the data. Although the two plots are similar in appearance there are differences, Figure \@ref(fig:iit-only) is the actual IIT measurements without regression applied and Figure \@ref(fig:iit-regress) is the predictions based on the regression analysis. A single variable regression model will take the form of $y_i=\beta_0+\beta_1x_i +\epsilon_i$ where $\beta_0$ is the intercept term, $\beta_1$ is the slope of the regression line and $\epsilon_i$ is the normally distributed error term to account for the difference between the predicted and observed values. The gray shaded area around the regression line in Figure \@ref(fig:iit-regress) is the confidence interval for model. The confidence interval is a measure of the uncertainty in average prediction. A 95% confidence interval is the range that would cover the *average* prediction 95% of the time. The light blue shaded area is the prediction interval which represents the range that would cover an individual measurement 95% of the time.
  
```{r iit_only model, message=FALSE}

#define the model type and engine for the analysis
lm_model <-
  linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

ys_iit_rec <- recipe(ten_ys ~ iitys_5 + mn + c ,
                      data = ndt_ten_comp) %>%
  step_center(all_predictors()) %>%
  # step_scale(all_predictors()) %>%
  step_interact(terms = ~ mn:c ) %>%
  prep() #retain data =TRUE is default


ys_bake <- bake(ys_iit_rec, new_data = NULL)

# fit the model
fitted_iit <-  fit(lm_model,
                formula = ten_ys ~ iitys_5 ,
                data = ys_bake)

# fitted_iit$fit# fitted model object

#extract coefficients
tidy_iit <- tidy(fitted_iit, conf.int=T) %>% 
  mutate(model ="IIT Only") 

#extract performance metrics
iit_glance <- glance(fitted_iit$fit) %>%
  mutate(model ="IIT Only") %>% 
  relocate(model)

#attach the preidcted values to the original data
ys_aug <- augment(fitted_iit$fit,interval = "prediction") %>%
  mutate(model="IIT Only")


```
 
The coefficients estimates and their uncertainties for the IIT only model are in shown in Table \@ref(tab:iit-only-coefs).  The standard error and confidence intervals reflect the range of uncertainties in the individual regression coefficients. The data for this model had a linear transformation known as centering applied. Centering is the process of by subtracting the mean from the individual values before performing regression. This is done to make the intercept term more meaningful. Without centering the data, the intercept of the IIT only model would represent the predicted YS when the IIT is zero which has no physical meaning. After centering the data, the intercept is interpreted as the predicted YS when the IIT is at the mean value and the slope coefficient is the relative change of the in the predicted value for an incremental change of one relative to the mean IIT. This linear transformation has no affect on the $R^2$ values or the fit of the model. For the IIT only model, it was done to make the intercept and coefficients more interpretable but when composition is included it becomes necessary for other reasons that will be discussed in the section on the IIT + Composition model.

```{r iit-only-coefs}
tidy_iit %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>% 
  mutate(across(.cols = all_numeric(), ~round(.x,3))) %>% 
  knitr::kable(caption = "Regression Estimates for IIT Only Model")

```
      
The model predicts an intercept of `r signif(tidy_iit$estimate[1],3)` and a slope of `r signif(tidy_iit$estimate[2],3)`. A slope of nearly 1 does not imply that the model is a perfect predictor as it does not account for the amount of scatter in the predictions.  The $R^2$ value was `r signif(iit_glance$r.squared,3)` Which would be considered a moderately strong correlation between the predicted values and the observed values but a fair amount of scatter still exists in the predictions. The observed vs.predicted for this model are shown in Figure \@ref(fig:iit-regress). The gray shaded area around the regression line is 95% confidence interval for the mean prediction at a given value.  The light blue shaded area around the regression line is the prediction interval which is the region that 95% of the individual predictions would be expected fall.  
  
```{r iit-regress, fig.cap="Regression Results"}
corners <- ys_aug %>% 
  summarise(x=c(min(.fitted),min(.fitted), max(.fitted),max(.fitted)), 
            y = c(min(.lower),min(.upper),max(.upper), max(.lower))) 

ys_aug %>%
  # pivot_longer(cols = c(.fitted_iit, .fitted_comp)) %>%
  ggplot(aes(y = ten_ys, x = .fitted)) +
  geom_jitter(width = 0.1, 
              height = 0.1,
              alpha = 0.65, 
              col = 'midnightblue') +
  coord_obs_pred() +
  geom_abline(lty = 2, col = 'grey50') +
  geom_smooth(method = "lm", se = T, col='orange') +
   geom_polygon(aes(x=x,y=y), corners, fill='steelblue2', alpha=0.1)+
  # geom_ribbon(aes(ymin = .lower, ymax=.upper), fill='steelblue2', alpha=0.2)+
  scale_color_brewer(type = "div", palette = "Set1") +
  labs(title = "IIT Only Model Observed vs. Predicted",
       x = "Predicted Tensile (ksi)",
       y = "Observed Tensile (ksi)")

```
  
The model has significant residuals at the higher predicted values. At a predicted yield of 70 ksi the range of residuals is between 57 and 83 ksi. This implies that there are other influences on the yield strength that are not included in the model and the $R^2$ value indicates that greater than 30% of the variance isn't accounted for by the model. This led to the investigation of other predictors that might be of use.  

## Motivation {-}
In order for other predictors to be useful for augmenting the IIT model, it needed to be able to be associated with the same samples that the IIT measurement were taken from. The pool of data available for this is the element compositions in the ECA2 data set. Since certain elements are known to influence the YS of steel, the wt percent of several elements were used as predictors in addition to the IIT measurements. For consideration as a candidate for modeling, the element needed to be one that can be reliably measured with NDE methods and be available throughout the range of pipeline steel grades and vintages. Some elements have significant impact on tensile strength but are difficult to measure with NDE or are only normally used in certain grades. An example of this would be Titanium which has a significant influence on YS but is typically only used in more modern, high-grade steels. Another example of an element that was considered but eventually rejected was Phosphorus which has known effects but are difficult to measure accurately with NDE. 
  
After several exploratory models it was decided that Manganese (Mn) and Carbon (C) had the characteristics of significantly affecting the YS of steel and can be reliably measured with NDE methods. Figure \@ref(fig:residual-mnc) shows the model residuals (observed - predicted) for the IIT only model versus Mn and C. There are noticeable trends in the residuals for both elements that indicate that they could be beneficial in improving the model. 
  
```{r residual-mnc, fig.cap="Residuals Plot for IIT Model", fig.width=6.5}
mn_resid <- ys_aug %>% 
  bind_cols(mn =ndt_ten_comp$mn, c = ndt_ten_comp$c) %>% 
  ggplot(aes(mn, .resid))+
  geom_point(col='orangered2')+
  geom_smooth(method = "lm", se=F)+
  labs(title = "Residuals vs. Mn Content",
       y = "Residuals",
       x = "Mn (wt%)")+
  theme(plot.title = element_text(size=14))
  
c_resid <- ys_aug %>% 
  bind_cols(mn =ndt_ten_comp$mn, c = ndt_ten_comp$c) %>% 
  ggplot(aes(c, .resid))+
  geom_point(col='maroon')+
  geom_smooth(method = "lm", se=F)+
  labs(title = "Residuals vs. C Content",
       y = "Residuals",
       x = "C (wt%)")+
  theme(plot.title = element_text(size=14))
  
mn_resid + c_resid +plot_annotation(caption = "IIT Only Model")


```
  
The relationship between Mn and C relative to the observed yield strength will be examined next. Figure \@ref(fig:mnc-plot) is a plot of the yield strength vs. the weight percent of C and Mn.  This demonstrates that additional C or Mn have opposite trends on the yield strength and therefore create what is called an interaction in a linear model, this indicates that the effect of one is not constant relative to the other. Because of this combined effect, the model will need to include an interaction term of Mn x C as well as the the contribution of Mn and C individually. The negative correlation of Mn and C 

When interaction between terms are used in a linear regression model it creates a multicolinearity where one or more of the predictors are correlated to other predictors violating the assumption of independence of predictors. But to counteract this problem the data was centered as discussed in the IIT Only model. In addition, this method removes the multicollinearity produced by interaction and higher-order terms, but it has the added benefit of not changing the interpretation of the coefficients. If you subtract the mean, each coefficient continues to estimate the change in the mean response per unit increase in *X* when all other predictors are held constant.  

Figure \@ref(fig:mnc-plot) shows the correlation between the two elements and yield strength but this should not be construed as causation. C is used to increase ultimate tensile strength but since it does so at the expense of ductility and toughness the higher yield strength steels have lower C content and use other elements to increase the strength. So it has a negative correlation to yield strength but indirectly so.  

```{r mnc-plot, fig.cap="Ys vs. Mn and C"}
ndt_ten_comp %>%
  pivot_longer(cols = mn:c) %>%
  ggplot(aes(value, ten_ys)) +
  geom_jitter(
    aes(col = name),
    width = 0.1,
    height = 0.1,
    alpha = 0.6,
    show.legend = F
  ) +
  facet_wrap( ~ name, scales = "free_x") +
  geom_smooth(method = "lm", se = F) +
  labs(title = "Correlation of Mn & C to YS",
       x="wt%",
       y = "Destructive YS (ksi)")


```
  
## Model with Composition {-}
This next section will present the results of the model with the IIT predicted yield strength along with Mn and C content as additional predictors. Since it is known that the effect on YS of Mn and C is not constant as shown in Figure \@ref(fig:mnc-plot) the model needs to account for this. This is done through the interaction term of Mn times C. 
  
The results of this model are shown in Figure \@ref(fig:fitted2).  
  
```{r fitted2, fig.cap="IIT + Compostion Model"}
fitted2 <-  fit(lm_model,
                formula = ten_ys ~ iitys_5 + mn * c  ,
                data = ys_bake) 

ys_glance2 <- glance(fitted2$fit) %>% mutate(model= "IIT + Composition")

ys_tidy2 <- tidy(fitted2,conf.int=T) %>% 
  mutate(model = "IIT + Comp")

ys_aug2 <- augment(fitted2$fit,interval = "prediction") %>%
  mutate(model="IIT + Comp")

corners2 <- ys_aug2 %>%
  summarise(x = c(min(.fitted), min(.fitted), max(.fitted), max(.fitted)),
            y = c(min(.lower), min(.upper), max(.upper), max(.lower))) 
  
ys_aug2 %>%
  # pivot_longer(cols = c(.fitted_iit, .fitted_comp)) %>%
  ggplot(aes(y = ten_ys, x = .fitted)) +
  geom_jitter(width = 0.1, 
              height = 0.1,
              alpha = 0.65, 
              col = 'midnightblue') +
  coord_obs_pred() +
  geom_abline(lty = 2, col = 'grey50') +
  geom_smooth(method = "lm", se = T, col='orange') +
  # geom_ribbon(aes(ymin = .lower, ymax=.upper), fill='steelblue2', alpha=0.2)+
  geom_polygon(aes(x=x,y=y), corners2, fill='steelblue2', alpha=0.2)+
  scale_color_brewer(type = "div", palette = "Set1") +
  labs(title = "IIT + Compostion Model",
       x = "Predicted Tensile (ksi)",
       y = "Observed Tensile (ksi)",
       caption = "YS ~ IIT + Mn + C + Mn * C")
  

```
  
## Lab vs. NDE Composition  {-}
In this next section, we will explore if the composition arrived at from a lab vs. NDE makes a difference in the model. To do this, the data was split by composition method and a nested model was created that evaluated the two groups of data in parallel. After fitting the model to the two sets of data the performance metrics were extracted and shown in Table \@ref(tab:nested-model). The difference in performance of the two models is not significantly different. The $R^2$ parameter is essentially the same and the standard deviation of the residuals (sigma) is slightly higher for the NDE. The NDE data set was about 15% smaller than the Lab data and if the uncertainty of the two standard deviations were accounted for the differences in the two are statistically indistinguishable from zero. To demonstrate this, a Bayesian simulation of the two models' residuals was undertaken and the results are shown in Figure  \@ref(fig:mcmc-sim).  Since the 95% credible interval of the differences in standard deviations includes zero it is can be judged that the differences seen in the model results are not significant enough to be able to say that they came from different populations, i.e. that the models are likely to produce statistically different populations of residuals based on this test data. That is not saying they are the same but only that it cannot be excluded that they are because the regions of uncertainty overlap.
  
A plot of the two models' predictions are shown in Figure \@ref(fig:lab-nde-plot). There are no identifiable visual trends in the two sets of predictions that make them significantly different.  
  
```{r nested-model}

ndt_ten_comp2 <- ndt %>%
  full_join(comp2,
            by = c("group", "feature")) %>%
  full_join(tensile,
            by = c("group", "feature")) %>%
  drop_na()

ys_labnde_rec <- recipe(ten_ys ~ iitys_5 + mn + c ,
                      data = ndt_ten_comp2) %>%
  step_center(all_predictors()) %>%
  # step_scale(all_predictors()) %>%
  step_interact(terms = ~ mn:c ) %>%
  prep()  #retain data =TRUE is default
  

labnde_bake <- bake(ys_labnde_rec,new_data = NULL) %>% 
  bind_cols(lab_nde = ndt_ten_comp2$lab_nde)

ys_lab_nde <- labnde_bake %>%
  # select(CVN_US_FS, pdp, ags, c,s, mli, lab_nde) %>%
  nest(c(-lab_nde)) %>%
  mutate(fit = map(data, ~lm(ten_ys ~ iitys_5 + mn * c, data = .x)),
         glanced = map(fit, glance),
         tidied = map(fit, tidy),
         aug = map(fit, augment)
         )


ys_lab_nde %>%
  select(glanced) %>%
  unnest(glanced) %>%
  mutate(method=c("Lab", "NDE")) %>%
  relocate(method) %>%
  select(method:sigma,-p.value, deviance) %>%
  mutate(across(.cols = c(2:5), ~ round(.x,2))) %>%
  knitr::kable(digits = 2,
               caption = "Lab vs. NDE Metrics")


```
  
    
```{r mcmc-sim, fig.width= 6.5, fig.cap = "MCMC Results", cache=TRUE}
library(BEST)
aug <- ys_lab_nde %>% unnest(aug)

labnde_best <- BESTmcmc(aug$.resid[aug$lab_nde=="lab"], aug$.resid[aug$lab_nde=="nde"],numSavedSteps = 5e3)

plotPost(labnde_best$sigma1 - labnde_best$sigma2, 
         credMass = 0.95, 
         compVal = 0, 
         showCurve = FALSE, 
         xlab = bquote(sigma[1] - sigma[2]), 
         cex.lab = 1.75, 
         main = "Difference of Std. Dev.s", 
         col = "skyblue", 
         showMode = TRUE)

```

  
```{r lab-nde-plot, fig.cap="Lab vs. NDE Predictions"}
ys_lab_nde %>% 
  unnest(aug) %>% 
  ggplot(aes(y = ten_ys, x = .fitted)) +
  geom_jitter(width = 0.1, 
              height = 0.1,
              alpha = 0.65, 
              aes(col = lab_nde)) +
  coord_obs_pred() +
  geom_abline(lty = 2, col = 'grey50') +
  scale_color_brewer(type = "div", palette = "Set1") +
  labs(title = "IIT + Compostion Model",
       subtitle = "Split by Lab vs. NDE",
       x = "Predicted Tensile (ksi)",
       y = "Observed Tensile (ksi)",
       caption = "YS ~ IIT + Mn + C + Mn * C",
       col="Lab vs. NDE")
  


```

The estimates of the coefficients for this model and for comparison, the values for the IIT Only model are tabulated in Table \@ref(tab:iitpc-coefs) along with their uncertainties. The high and low confidence intervals reflect the 95% range of uncertainty for each variable.
  
```{r iitpc-coefs}

tab <- ys_tidy2 %>%
  bind_rows(tidy_iit) %>% 
  select(term, estimate, std.error, conf.low, conf.high, model) %>%
  mutate(across(.cols = all_numeric(), ~ round(.x, 2)))

  knitr::kable(tab,caption = "Regression Estimates for IIT + Comp. Model")

```


## Model Comparison {-}
The significant residuals at the higher yield strengths of the IIT only model has largely been counteracted and the $R^2$ has increased from `r signif(iit_glance$adj.r.squared,3)` to `r signif(ys_glance2$adj.r.squared,3)`. Figure \@ref(fig:resid-plot) is a plot of the residuals (observed minus predicted) for both models with a loess smoothed line. The loess smoother is localized regression that shows the change in the trend throughout the range of the data set.  The IIT + composition model has a far more stable pattern of residuals with the average at or near zero through about 70 ksi. This means that the predictions are roughly equal in the tendency to over as well as under predict the destructive YS and that there isn't a bias throughout the model's range. Whereas the "IIT only" model demonstrates different biases at different parts of the range swinging from positive to negative at different locations. The residual pattern of the IIT + composition is more consistent with the assumptions for linear regression of normally distributed errors with a constant variance. 
  
```{r resid-plot, fig.width=6.5 ,fig.cap="Comparison Plot of Residuals"}

bind_rows(ys_aug, ys_aug2) %>% 
  ggplot(aes(.fitted, .resid))+
  geom_point(aes(col=model))+
  geom_smooth(method = "loess", se=F, aes(col=model))+
  scale_color_brewer(type = "div", palette = "Set1")+
  labs(title = "Model Residuals",
       x = "Predicted",
       y = "Residuals")

```
  
When comparing competing models and attempting to determine whether the increased complexity of additional predictors is actually a benefit the *adjusted* $R^2$, the AIC and BIC values allow for judging the complexity relative to any increase in performance of the model.  The adjusted $R^2$ is an adjustment to the regular $R^2$ that "penalizes" the value for using multiple predictors. When comparing models fitted by maximum likelihood to the same data, the smaller the AIC or BIC, the better the fit. The model that uses the composition in addition to the IIT measurement has a higher adjusted R-Squared in addition to a lower AIC and BIC indicating better performance than the IIT alone. The deviance parameter is the sum of the squares of the residuals. This also is a metric for measuring the goodness of fit.  It is calculated by taking the square of the difference between the observed and predicted values. Since the residuals are squared, model errors are penalized at a much higher rate than a simple sum of the residuals and provides a more robust comparison between competing models. In reviewing the results for both models tabulated in Table \@ref(tab:metrics), the IIT + Composition model has shown improvement in every metric across the board. Specifically it has reduced the deviance relative to the IIT Only model by 54%.  
  
Additional model diagnostics can be found in the appendix.

```{r metrics}
tab <- bind_rows(iit_glance, ys_glance2) %>%
  select(model,
         r.squared,
         adj.r.squared,
         sigma,
         AIC,
         BIC,
         deviance)

  knitr::kable(tab, digits = 2, caption = "Comparison of Performance Metrics")

```
  
## Uncertainty {-}
In order to test the sensitivity of the two proposed models to variations in the IIT and composition, 10,000 random samples were drawn from a normal distribution based on the mean and standard deviation of the NDE measurements for a single pipe sample.  Then this data was applied to the YS prediction models. The results are shown in Figure\@ref(fig:uncertain) which demonstrates that the variation in the predicted YS for the composition model was within about $\pm$ 7.5 ksi of the mean destructive YS and the average predicted value was within approximately 0.6 ksi which is about 1% of the YS. The IIT Only model shows significantly more spread and has a bias of approximately 4 ksi in the direction of overprediction. This shows that the IIT + composition model does not produce large swings in predicted YS based on the typical variation in measurements for IIT and composition and the mean value is centered on the destructive YS.

Since these predictions are intended as inputs to a grade prediction machine learning model that also will use Mn and C as predictors, there is concern that there could be a propagation of uncertainty. To test this concern the values from this uncertainty analysis were used in the Random Forest Grade Prediction model and 88% of the samples came back with the same grade as the original data. In 9% of the cases it predicted one grade higher and 3% of the cases it predicted two grades higher. This demonstrates a very robust predictions from the grade prediction model given that 10,000 samples were drawn, some of the compositions were likely 3 standard deviations or more from the mean. These extreme composition samples are a very unlikely scenario in practice where multiple measurements are taken and aggregated. In addition, since the IIT + Composition model generates more accurate YS predictions it is likely to improve the grade predictions over the IIT Only model.

```{r uncertain, fig.cap="Uncertainty in YS Predictions", cache=TRUE, fig.width =6.5}

c_m <- 0.268
c_sd <- 0.024

mn_m <- 0.594
mn_sd <-  0.05


iit_m <- 50.39
iit_sd <- 6.22

n <- 1e4

tensile_mean <- mean(c(46.3,44.1,49.4,49.3,50.1,48.4))

c <- rnorm(n = n,mean = c_m,sd = c_sd)

mn <- rnorm(n,mean = mn_m, sd = mn_sd)

iit <- rnorm(n, mean = iit_m,sd = iit_sd)

avg <- bind_cols(c = c_m, mn = mn_m, iit = tensile_mean)

dip <- tibble(c, mn, iit) %>%
  bind_rows(avg) %>%
  rename(iitys_5 = iit)

# bake the predictors to use in fitted model
dip_bake <- bake(ys_iit_rec, new_data = dip) 

## Since this is being used for the RF model combine with non-scaled predictors
ys_dip_comp <- predict(fitted2, new_data = dip_bake) %>%
  bind_cols(dip) %>%
  rename(ys_ksi = .pred) %>%
  mutate(model="IIT + Comp.")

ys_dip <- predict(fitted_iit, new_data = dip_bake) %>%
  bind_cols(dip) %>%
  rename(ys_ksi = .pred) %>% 
  mutate(model="IIT Only") %>% 
  bind_rows(ys_dip_comp)

## confidence interval of the predicted YS
ys_up <- mean(ys_dip_comp$ys_ksi) + qnorm(0.975) * sd(ys_dip_comp$ys_ksi)
ys_low <- mean(ys_dip_comp$ys_ksi) - qnorm(0.975) * sd(ys_dip_comp$ys_ksi) 

## Plot Density
ys_dip %>%
  ggplot(aes(ys_ksi)) +
  geom_density(aes(fill=model), alpha = 0.4) +
  geom_vline(
    xintercept = tensile_mean,
    lty = 2,
    lwd = 0.9,
    col = 'red'
  ) +
  geom_errorbarh(aes(xmax = ys_up,
                     xmin = ys_low,
                     y = 0.01),
                 height = 0.01,
                 size = 1) +
  geom_point(aes(x = mean(ys_dip_comp$ys_ksi),
                 y = 0.01),
             size = 3) +
  annotate(
    "text",
    x = tensile_mean,
    y = 0.045,
    label = "Destruct. Tensile Avg.",
    angle = -90,
    vjust = -0.5,
    size = 4
  ) +
  # annotate(
  #   "text",
  #   x = 50,
  #   y = 0.01,
  #   label = "95% Pred. Int.",
  #   vjust = -0.6,
  #   hjust = 0,
  #   size = 4
  # ) +
  labs(
    title = "Predicted YS Uncertainty for Two Models",
    x = "Yield Strength (ksi)",
    y = "Probability Density",
    fill="Model",
    # caption = "Pred. Int. = Prediction Interval"
  ) +
  scale_x_continuous(breaks = scales::pretty_breaks())+
  theme(legend.position = c(0.80,0.75))

```
  
The model sensitivity study looks at the uncertainty from the standpoint of that the model would not change with additional data and how the variation in the variables affect the predictions. The other form of uncertainty is the potential errors of the model itself and how that might change with additional data.  This is accounted for in the standard errors of the regression coefficients. These reflect the uncertainty in the coefficients based on the the data that the model was trained on.  The interaction term of Mn X C has the highest standard error because it is the product of two very small numbers, making small changes in either variable significant. Although the standard error of approximately 25 is large when compared to the others, the net effect is small since the 95% of the deviation from the average is less than $\pm$ 0.05.  This implies that if everything else was held constant, the uncertainty in the model would be about (0.05 x 25) 1.25 ksi.
  
```{r coef-table }

knitr::kable(tidy(fitted2)[, 1:3],
                  digits = 2,
                  caption = "Model Parameters for IIT + Composition")

```
   
## Training and Test Evaluation {-}
In machine learning it is standard practice to split the data into two parts.  A training and a test set, where as the names imply the model is trained on the training set and the performance is evaluated based on the held-out test set that the model hasn't been trained on. This gives a better prediction of how the model will perform going forward on new data and helps avoid over-fitting where the model is fit specifically to one set of data and performs poorly when used against the validation data that it wasn't trained on. The test set has a tendency to underperform the training since the model wasn't trained on it. The original data set is rather small to split but to get a sense of the out of how the proposed addition of composition to the IIT model the data was randomly split 85% into training and 15% into testing and the model retrained. The results the model against the validation data are shown in \@ref(tab:train-testtab) which shows an $R^2$ of 0.82 which is inline with the original model with all the data included. But with only 13 data points in the test data this could be due to a favorable random draw for test set.

```{r train-test}
set.seed(421)
ys_split <- initial_split(ndt_ten_comp, prop = 0.85)

train <- training(ys_split)
test <- testing(ys_split)


lm_workflow <- workflow() %>%
  # add the recipe
  add_recipe(ys_iit_rec) %>%
  # add the model
  add_model(lm_model)

fit_model <- lm_workflow %>% 
  fit(data=train) 

```


```{r train-testtab}
fit_model %>%
  predict(test) %>% 
  bind_cols(test,.) %>%
  metrics(ten_ys, .pred) %>% 
  select(-.estimator) %>% 
  knitr::kable(digits = 2, caption = "Test Set Regression Performance")

```
  
## Leave One Out Cross Validation {-}
The training/test split is the standard way of evaluating a machine learning model but with this small of a data set, depending on the random split the performance metrics can have a large variance due to the small sample size. The more robust way to evaluate a regression model such as this would be with leave-one-out (LOO) cross validation. In this process, a Bayesian regression model is fit repeatedly equal to the number of data points. With each iteration one data point is excluded from the model and then the log-probability score is calculated. After all the models have been evaluated the scores are summed. This cross validation log score by itself is not predictive of model performance but is useful when comparing multiple models with different parameters such as this. An important thing to note in the interpretation of log scores is that they are based on the log of a probability which by definition is between zero and one, a log of a higher probability will produce a less negative number than a lower probability. Therefore a lower absolute value of the log score is desirable. 

The LOO results for both models are shown in Table \@ref(tab:kfold-cv). elpd_loo is the estimated log score along with its standard error to show the uncertainty in the score. p_loo is effective number of parameters and looic is the LOO information criterion which is a measure of deviance of the model. The columns appended with _diff are the difference between the two models. After completing the cross validation comparison, the IIT Only model shows a log score that is more negative by over 21 indicating that the IIT + Composition model outperformed the IIT only model.

```{r kfold-cv, cache=TRUE}
library(rstanarm)
options(mc.cores = parallel::detectCores())

iit_only <- stan_glm(ten_ys ~ iitys_5,
                     family = "gaussian",
                     ys_bake)

iit_comp <-
  stan_glm(ten_ys ~ iitys_5 + mn * c,
           family = "gaussian",
           ys_bake)


loo_iit <- loo(iit_only)

loo_comp <- loo(iit_comp)

knitr::kable(data.frame(loo_compare(loo_iit, loo_comp)), 
             digits = 1,
             caption = "LOO Cross Validation Results")

```
  
## UTS Discussion {-}  
A similar model for ultimate tensile strength (UTS) was fit using the IIT measurement with Mn and C (but no the interaction). This is because C is positively correlated to UTS unlike YS negating the necessity of the interaction term. The metrics are shown in Table \@ref(tab:uts). The adjusted $R^2$ shows a slightly lower value than the YS model but still reasonably good give the uncertainty in several of the parameters.  It might be possible to find other elements that are suitable predictors but that is beyond the scope of this document.

```{r uts}
uts_rec <- recipe(ten_uts ~ iitys_5 + mn + c ,
                  data = ndt_ten_comp) %>%
  step_center(all_predictors()) %>%
  # step_scale(all_predictors()) %>%
  step_interact(terms = ~ mn:c) %>%
  prep() #retain data =TRUE is default

uts_bake <- bake(uts_rec, new_data = NULL)

# fit the model
fitted_uts <-  fit(lm_model,
                   formula = ten_uts ~ iitys_5 + mn + c ,
                   data = uts_bake)

glance(fitted_uts) %>% 
  select(r.squared, adj.r.squared, sigma) %>% 
  knitr::kable(digits = 2, caption ="UTS Model Metrics")

```
  
```{r uts-plot,fig.cap="UTS Predictions"}
uts_aug <- augment(fitted_uts$fit,new_data = ndt_ten_comp,interval = "prediction") 
  
  corners_u <- uts_aug %>%
  summarise(x = c(min(.fitted), min(.fitted), max(.fitted), max(.fitted)),
            y = c(min(.lower), min(.upper), max(.upper), max(.lower))) 
  
uts_aug %>%
  # pivot_longer(cols = c(.fitted_iit, .fitted_comp)) %>%
  ggplot(aes(y = ten_uts, x = .fitted)) +
  geom_jitter(width = 0.1, 
              height = 0.1,
              alpha = 0.65, 
              col = 'midnightblue') +
  coord_obs_pred() +
  geom_abline(lty = 2, col = 'grey50') +
  geom_smooth(method = "lm", se = T, col='orange') +
  # geom_ribbon(aes(ymin = .lower, ymax=.upper), fill='steelblue2', alpha=0.2)+
  geom_polygon(aes(x=x,y=y), corners_u, fill='steelblue2', alpha=0.2)+
  scale_color_brewer(type = "div", palette = "Set1") +
  labs(title = "IIT + Compostion Model for UTS",
       x = "Predicted Tensile (ksi)",
       y = "Observed Tensile (ksi)",
       caption = "UTS ~ IIT + Mn + C ")
  

```
  
## Future Work {-}  
The inclusion of Mn and C show significant improvements over the IIT Only model. In addition, the inclusion of microstructure is showing potential additional improvements. The subset of samples that have microstructure and composition is about half of the amount of data that was used in the IIT + Composition model. This reduced data set was used to fit a third model that included microstructure variable of mean linear intercept as well as composition.  The results show an $R^2$ of 0.91. Again this is with the caveat that it is fit using half the number of data points and the improvement in correlation could be due to the reduced number of points needed to fit. However, it does show some promise for improvement over the composition based model.
  
```{r ms-comp}

MICRO <-
  read_excel(
    "C:\\Users\\Joel\\OneDrive - RSI Pipeline Solutions\\PGE\\charpy\\Microstructure_Report\\Microstructure_Data_Collection_Online_2021_01_05_S5CJ.xlsx",
    sheet = "Collection2"
  ) %>%
  mutate(
    Sample_Type = str_replace(Sample_Type, "-", "_"),
    name = paste0(Group, " ", Feature),
    Path = str_remove(string = Path, pattern = ".jpg"),
    Path = str_remove(string = Path, pattern = ".tif")
  ) %>% 
  janitor::clean_names() %>% 
  rename(mli = mean_linear_intercept,
         pdp = pct_dark_phase) %>% 
  group_by(group, feature) %>% 
  summarise(mli = mean(mli, na.rm=T),
            pdp = mean(pdp, na.rm=T))

micro_nde_comp <- ndt %>%
  full_join(comp,
            by = c("group", "feature")) %>%
  full_join(tensile,
            by = c("group", "feature")) %>%
  full_join(MICRO,
            by = c("group", "feature")) %>%
  drop_na()


ys_rec <-
  recipe(ten_ys ~ iitys_5+ c + mn + si + mli+pdp,
         data = micro_nde_comp) %>%
  step_center(all_predictors()) %>%
  # step_scale(all_predictors()) %>%
  step_interact(terms = ~ c(mn:c, c:si, si:mli)) %>%
  prep() #retain data =TRUE is default

ys_bake <- bake(ys_rec, new_data = NULL)

# fit the model
fitted_mli <-  fit(lm_model,
                 formula = ten_ys ~ iitys_5 + mn *c+si:mli ,
                 data = ys_bake)

glance(fitted_mli) %>%
  select(r.squared, adj.r.squared, sigma, deviance) %>% 
  knitr::kable(digits = 2, caption = "YS Model With Mean Linear Intercept")

```


## Conclusion {-}
The IIT measurements combined with the composition remove the bias out of the IIT only model and significantly reduces the variance in the residuals to make a superior model. It also shows less uncertainty and reduces the model variance by half compared to the IIT Only model. The composition model without exception has better metrics and lower uncertainty at every level. There is not a single metric where the IIT Only performs better than the IIT + Composition model. It produces higher $R^2$ values and lower standard errors. Also, when split into a training and test data set the validation showed equal performance as when the entire data set is used to train the model. This provides reasonable assurance that future predictions will be accurate as the training data suggest.

In addition since the Mn and C can both be reliably measured with NDE they make excellent additional predictors and should be used to adjust IIT predictions.

## Appendix {-}
The IIT Only model shows deviance from a normal distribution in the far right tails as shown in Figure \@ref(fig:modelsum-iit) as well as some heterogeneity of the variance as well.  
  
```{r modelsum-iit, fig.cap="IIT Only Diagnostics", fig.height=6, fig.width=6.5}

check_model(fitted_iit)

```
  
The variance of the IIT + Composition model follows a normal distribution and a more uniform distribution of residuals which is more consistent with the regression assumptions.  
```{r modelsum-comp,fig.cap = "IIT + Composition Diagnostics" , fig.height=8.5, fig.width=6.5}

check_model(fitted2)
```

